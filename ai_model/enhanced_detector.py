#!/usr/bin/env python3
"""
Enhanced DeepFake Detection Model - Priority 1 MVP Implementation
Combines CLIP zero-shot detection and LAA-Net for robust ensemble detection.

This implementation focuses on:
1. CLIP zero-shot detection - Highly generalizable, excellent for modern diffusion-based deepfakes
2. LAA-Net - Quality-agnostic artifact attention model (CVPR 2024)

The ensemble fuses their predictions for robust detection.
"""

import os
# CRITICAL: Force CPU mode BEFORE any imports (must be first)
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all TensorFlow messages
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
# Disable TensorFlow GPU completely
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
os.environ['TF_USE_LEGACY_KERAS'] = '1'

import cv2
import torch
# Disable CUDA if explicitly requested
if os.getenv('CUDA_VISIBLE_DEVICES') == '':
    # Monkey patch to force CPU
    original_cuda_available = torch.cuda.is_available
    torch.cuda.is_available = lambda: False

# Suppress warnings before importing open_clip
import warnings
warnings.filterwarnings('ignore')

# Import open_clip with stderr suppression
import contextlib
import io
stderr_suppressor = io.StringIO()

try:
    with contextlib.redirect_stderr(stderr_suppressor):
        import open_clip
except Exception:
    # If import fails, try without suppression
    import open_clip
from PIL import Image
import numpy as np
import sys
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# Try to import LAA-Net components (will be available after submodule setup)
try:
    # Adjust these imports based on actual LAA-Net structure
    # Example paths (to be updated after submodule setup):
    # sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'external', 'LAA-Net'))
    # from models import LAANet  # Replace with actual import
    # from utils.face_utils import preprocess_face  # Replace with actual import
    LAA_NET_AVAILABLE = False
except ImportError:
    LAA_NET_AVAILABLE = False

# Face detection - MTCNN will be lazy-loaded to prevent TensorFlow from importing at module level
# This prevents CUDA initialization errors on CPU-only servers
MTCNN_AVAILABLE = None  # Will be determined on first use


class FaceDetector:
    """Face detection and cropping utility for LAA-Net preprocessing."""
    
    def __init__(self, method: str = 'auto'):
        """
        Initialize face detector.
        
        Args:
            method: 'mtcnn', 'haar', or 'auto' (tries MTCNN first, falls back to Haar)
        """
        self.method = method
        self.mtcnn = None
        self.haar_cascade = None
        self._mtcnn_available = None
        
        # Try MTCNN only if requested, with complete TensorFlow suppression
        if method in ['auto', 'mtcnn']:
            self._try_mtcnn()
        
        # Fall back to Haar if MTCNN not available or failed
        if self.method == 'haar' or (method == 'auto' and self.mtcnn is None):
            self._init_haar()
    
    def _try_mtcnn(self):
        """Try to load MTCNN with complete TensorFlow/CUDA suppression."""
        global MTCNN_AVAILABLE
        
        # Check if we've already determined MTCNN availability
        if MTCNN_AVAILABLE is False:
            return  # Already tried and failed
        
        try:
            # CRITICAL: Suppress TensorFlow/CUDA errors during MTCNN import
            # Redirect both stderr and stdout to completely hide TensorFlow initialization
            import contextlib
            import io
            import sys
            
            old_stderr = sys.stderr
            old_stdout = sys.stdout
            suppress_io = io.StringIO()
            
            # Temporarily redirect all output
            sys.stderr = suppress_io
            sys.stdout = suppress_io
            
            try:
                # Set TensorFlow env vars before import
                os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
                os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'
                
                # Now try to import MTCNN (this will import TensorFlow)
                from mtcnn import MTCNN
                
                # Restore output immediately after import
                sys.stderr = old_stderr
                sys.stdout = old_stdout
                
                # If import succeeded, try to create MTCNN instance
                try:
                    # Suppress output during MTCNN initialization too
                    sys.stderr = suppress_io
                    sys.stdout = suppress_io
                    self.mtcnn = MTCNN()
                    sys.stderr = old_stderr
                    sys.stdout = old_stdout
                    
                    self.method = 'mtcnn'
                    MTCNN_AVAILABLE = True
                    self._mtcnn_available = True
                    return  # Success!
                except Exception as init_e:
                    sys.stderr = old_stderr
                    sys.stdout = old_stdout
                    # MTCNN import succeeded but initialization failed
                    MTCNN_AVAILABLE = False
                    self._mtcnn_available = False
                    
            except ImportError:
                # Restore output before checking availability
                sys.stderr = old_stderr
                sys.stdout = old_stdout
                MTCNN_AVAILABLE = False
                self._mtcnn_available = False
            except Exception as e:
                # Restore output before handling error
                sys.stderr = old_stderr
                sys.stdout = old_stdout
                # Any other error during import (including CUDA errors from TensorFlow)
                MTCNN_AVAILABLE = False
                self._mtcnn_available = False
                
        except Exception as e:
            # Ultimate fallback - something went very wrong
            MTCNN_AVAILABLE = False
            self._mtcnn_available = False
            # Don't print error - we'll fall back to Haar silently
    
    def _init_haar(self):
        """Initialize Haar cascade face detector."""
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        if os.path.exists(cascade_path):
            self.haar_cascade = cv2.CascadeClassifier(cascade_path)
            self.method = 'haar'
        else:
            print("Warning: Haar cascade not found. Face detection may fail.")
    
    def detect_face(self, image: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """
        Detect face in image and return bounding box.
        
        Args:
            image: Input image as numpy array (RGB or BGR)
            
        Returns:
            (x, y, w, h) bounding box or None if no face detected
        """
        if image is None or image.size == 0:
            return None
        
        # Convert to RGB if needed
        if len(image.shape) == 3 and image.shape[2] == 3:
            # Assume BGR if using OpenCV, convert to RGB for MTCNN
            if self.method == 'mtcnn' and self.mtcnn is not None:
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.dtype == np.uint8 else image
            else:
                image_rgb = image
        else:
            image_rgb = image
        
        if self.method == 'mtcnn' and self.mtcnn is not None:
            try:
                results = self.mtcnn.detect_faces(image_rgb)
                if results and len(results) > 0:
                    # Get the largest face
                    largest_face = max(results, key=lambda x: x['box'][2] * x['box'][3])
                    x, y, w, h = largest_face['box']
                    return (x, y, w, h)
            except Exception as e:
                print(f"MTCNN detection error: {e}")
        
        if self.method == 'haar' and self.haar_cascade is not None:
            try:
                # Convert to grayscale for Haar
                if len(image_rgb.shape) == 3:
                    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY) if image_rgb.shape[2] == 3 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                else:
                    gray = image_rgb
                
                faces = self.haar_cascade.detectMultiScale(
                    gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
                )
                if len(faces) > 0:
                    # Get the largest face
                    largest_face = max(faces, key=lambda x: x[2] * x[3])
                    x, y, w, h = largest_face
                    return (x, y, w, h)
            except Exception as e:
                print(f"Haar detection error: {e}")
        
        return None
    
    def crop_face(self, image: np.ndarray, bbox: Optional[Tuple[int, int, int, int]] = None, 
                  target_size: Tuple[int, int] = (224, 224), padding: float = 0.2) -> Optional[np.ndarray]:
        """
        Crop and resize face from image.
        
        Args:
            image: Input image
            bbox: Bounding box (x, y, w, h). If None, will detect face.
            target_size: Target size (width, height)
            padding: Padding factor around face (0.2 = 20% padding)
            
        Returns:
            Cropped and resized face image or None
        """
        if bbox is None:
            bbox = self.detect_face(image)
            if bbox is None:
                return None
        
        x, y, w, h = bbox
        
        # Add padding
        pad_w = int(w * padding)
        pad_h = int(h * padding)
        x = max(0, x - pad_w)
        y = max(0, y - pad_h)
        w = min(image.shape[1] - x, w + 2 * pad_w)
        h = min(image.shape[0] - y, h + 2 * pad_h)
        
        # Crop face
        face_crop = image[y:y+h, x:x+w]
        
        if face_crop.size == 0:
            return None
        
        # Resize to target size
        face_resized = cv2.resize(face_crop, target_size, interpolation=cv2.INTER_LINEAR)
        
        return face_resized


class EnhancedDetector:
    """
    Enhanced ensemble detector combining CLIP zero-shot and LAA-Net.
    
    This is the Priority 1 MVP implementation focusing on two strong,
    complementary components for effective deepfake detection.
    """
    
    def __init__(self, laa_weights_path: Optional[str] = None, 
                 device: Optional[str] = None,
                 clip_model_name: str = 'ViT-B-32',
                 clip_pretrained: str = 'laion2b_s34b_b79k'):
        """
        Initialize the enhanced detector.
        
        Args:
            laa_weights_path: Path to LAA-Net pre-trained weights (optional)
            device: Device to use ('cuda', 'cpu', or None for auto-detect)
            clip_model_name: CLIP model variant to use
            clip_pretrained: CLIP pretrained weights identifier
        """
        # Force CPU if CUDA_VISIBLE_DEVICES is set to empty
        if os.getenv('CUDA_VISIBLE_DEVICES') == '':
            self.device = 'cpu'
        else:
            self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        import logging
        logger = logging.getLogger(__name__)
        logger.info(f"ðŸ”§ Initializing EnhancedDetector on device: {self.device}")
        
        # Initialize face detector for LAA-Net preprocessing
        logger.info("ðŸ” Initializing face detector (MTCNN/OpenCV)...")
        self.face_detector = FaceDetector(method='auto')
        if self.face_detector.method == 'mtcnn':
            logger.info("âœ… MTCNN face detection initialized successfully")
        else:
            logger.info("â„¹ï¸  Using OpenCV Haar cascades for face detection (MTCNN unavailable or failed)")
        
        # === CLIP Zero-Shot Setup ===
        logger.info("ðŸ“¦ Loading CLIP model (ViT-B-32)...")
        try:
            # Force CPU device explicitly to avoid CUDA errors
            self.device = 'cpu'
            
            # Suppress stderr during model loading to hide CUDA errors
            import contextlib
            import io
            stderr_suppressor = io.StringIO()
            
            with contextlib.redirect_stderr(stderr_suppressor):
                self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
                    clip_model_name, pretrained=clip_pretrained
                )
                self.clip_model.to('cpu')  # Explicitly use CPU
                self.clip_model.eval()
            
            # Optimized prompts for real vs. fake detection
            # Tuned for modern diffusion-based and traditional deepfakes
            texts = [
                "a real photograph of a human face taken by a camera",
                "a fake, manipulated, or AI-generated deepfake face, possibly from diffusion models"
            ]
            self.text_tokens = open_clip.tokenize(texts).to('cpu')
            with torch.no_grad():
                self.text_features = self.clip_model.encode_text(self.text_tokens)
                self.text_features /= self.text_features.norm(dim=-1, keepdim=True)
            
            logger.info("âœ… CLIP model loaded successfully and ready for inference (CPU mode)")
        except Exception as e:
            # Check if it's a CUDA error - if so, try again with explicit CPU
            error_str = str(e).lower()
            if 'cuda' in error_str or 'cuinit' in error_str:
                logger.warning(f"âš ï¸  CUDA error during CLIP loading, retrying with explicit CPU mode...")
                try:
                    self.device = 'cpu'
                    import contextlib
                    import io
                    stderr_suppressor = io.StringIO()
                    
                    with contextlib.redirect_stderr(stderr_suppressor):
                        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
                            clip_model_name, pretrained=clip_pretrained
                        )
                        self.clip_model.to('cpu')
                        self.clip_model.eval()
                    
                    texts = [
                        "a real photograph of a human face taken by a camera",
                        "a fake, manipulated, or AI-generated deepfake face, possibly from diffusion models"
                    ]
                    self.text_tokens = open_clip.tokenize(texts).to('cpu')
                    with torch.no_grad():
                        self.text_features = self.clip_model.encode_text(self.text_tokens)
                        self.text_features /= self.text_features.norm(dim=-1, keepdim=True)
                    
                    logger.info("âœ… CLIP model loaded successfully in CPU mode after CUDA error")
                except Exception as e2:
                    logger.error(f"âŒ Error loading CLIP model even in CPU mode: {e2}")
                    raise
            else:
                logger.error(f"âŒ Error loading CLIP model: {e}")
                raise
        
        # === LAA-Net Setup ===
        self.laa_model = None
        self.laa_available = False
        
        if LAA_NET_AVAILABLE and laa_weights_path and os.path.exists(laa_weights_path):
            try:
                logger.info("ðŸ“¦ Loading LAA-Net model...")
                # TODO: Replace with actual LAA-Net loading code after submodule setup
                # Example:
                # from external.laa_net.models import LAANet
                # self.laa_model = LAANet(...)
                # self.laa_model.load_state_dict(torch.load(laa_weights_path, map_location=self.device))
                # self.laa_model.to(self.device)
                # self.laa_model.eval()
                # self.laa_available = True
                logger.info("â„¹ï¸  LAA-Net model loading will be implemented after submodule setup.")
            except Exception as e:
                logger.warning(f"âš ï¸  Could not load LAA-Net model: {e}")
                logger.info("Continuing with CLIP-only detection.")
        else:
            if not LAA_NET_AVAILABLE:
                logger.info("â„¹ï¸  LAA-Net not available (submodule not set up). Using CLIP-only detection.")
            elif not laa_weights_path:
                logger.info("â„¹ï¸  LAA-Net weights path not provided. Using CLIP-only detection.")
            else:
                logger.info(f"â„¹ï¸  LAA-Net weights not found at {laa_weights_path}. Using CLIP-only detection.")
        
        logger.info("âœ… EnhancedDetector initialization complete")
        
        # LAA-Net preprocessing transform (placeholder - will use actual transform from LAA-Net)
        # This should match LAA-Net's expected input format
        self.laa_transform = None  # Will be set when LAA-Net is available
    
    def extract_frames(self, video_path: str, num_frames: int = 16) -> List[Image.Image]:
        """
        Extract evenly spaced frames from video.
        
        Args:
            video_path: Path to video file
            num_frames: Number of frames to extract
            
        Returns:
            List of PIL Images
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Could not open video: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            cap.release()
            raise ValueError("Invalid video path or empty video.")
        
        interval = max(1, total_frames // num_frames)
        frames = []
        count = 0
        
        while cap.isOpened() and len(frames) < num_frames:
            ret, frame_bgr = cap.read()
            if not ret:
                break
            if count % interval == 0:
                frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame_rgb))
            count += 1
        
        cap.release()
        
        if not frames:
            raise ValueError("No frames extracted from video.")
        
        return frames
    
    def clip_detect_frames(self, frames: List[Image.Image]) -> float:
        """
        Run CLIP zero-shot detection on frames and return average fake probability.
        
        Args:
            frames: List of PIL Images
            
        Returns:
            Average fake probability (0=real, 1=fake)
        """
        fake_probs = []
        
        for frame in frames:
            try:
                image_input = self.clip_preprocess(frame).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.encode_image(image_input)
                    image_features /= image_features.norm(dim=-1, keepdim=True)
                    similarity = image_features @ self.text_features.T
                    probs = similarity.softmax(dim=-1)[0].cpu().numpy()
                    fake_prob = probs[1]  # Index 1 = fake prompt
                fake_probs.append(fake_prob)
            except Exception as e:
                print(f"CLIP detection error on frame: {e}")
                continue
        
        if not fake_probs:
            return 0.5  # Neutral if all frames failed
        
        return np.mean(fake_probs)
    
    def laa_detect_frames(self, frames: List[Image.Image]) -> float:
        """
        Run LAA-Net detection on frames (requires face cropping).
        
        Args:
            frames: List of PIL Images
            
        Returns:
            Average fake probability (0=real, 1=fake)
        """
        if not self.laa_available or self.laa_model is None:
            # Return neutral score if LAA-Net not available
            return 0.5
        
        fake_probs = []
        
        for frame in frames:
            try:
                # Convert PIL to numpy for face detection
                frame_np = np.array(frame)
                
                # Crop face
                face_crop = self.face_detector.crop_face(frame_np, target_size=(224, 224))
                if face_crop is None:
                    continue  # Skip if no face detected
                
                # Apply LAA-Net preprocessing transform
                # TODO: Replace with actual LAA-Net transform after submodule setup
                # Example:
                # if self.laa_transform:
                #     face_tensor = self.laa_transform(face_crop).unsqueeze(0).to(self.device)
                # else:
                #     # Fallback transform
                #     face_tensor = torch.from_numpy(face_crop).permute(2, 0, 1).float() / 255.0
                #     face_tensor = face_tensor.unsqueeze(0).to(self.device)
                #
                # with torch.no_grad():
                #     output = self.laa_model(face_tensor)
                #     fake_prob = torch.sigmoid(output).item()  # Assuming binary logit
                # fake_probs.append(fake_prob)
                
                # Placeholder: return neutral for now
                fake_probs.append(0.5)
                
            except Exception as e:
                print(f"LAA-Net detection error on frame: {e}")
                continue
        
        if not fake_probs:
            return 0.5  # Neutral if all frames failed
        
        return np.mean(fake_probs)
    
    def detect(self, video_path: str, num_frames: int = 16) -> Dict[str, Any]:
        """
        Main detection method: Ensemble fake probability.
        
        Args:
            video_path: Path to video file
            num_frames: Number of frames to sample for detection
            
        Returns:
            Dictionary with detection results:
            - ensemble_fake_probability: Combined score (0=real, 1=fake)
            - clip_fake_probability: CLIP-only score
            - laa_fake_probability: LAA-Net score (or 0.5 if unavailable)
            - is_deepfake: Boolean prediction
            - method: Detection method used
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        # Extract frames
        frames = self.extract_frames(video_path, num_frames)
        
        # Run CLIP detection
        clip_prob = self.clip_detect_frames(frames)
        
        # Run LAA-Net detection (or placeholder)
        laa_prob = self.laa_detect_frames(frames)
        
        # Simple ensemble (average) - can be enhanced with adaptive weighting later
        if self.laa_available:
            ensemble_prob = (clip_prob + laa_prob) / 2.0
            method = 'ensemble_clip_laa'
        else:
            # CLIP-only if LAA-Net not available
            ensemble_prob = clip_prob
            method = 'clip_only'
        
        return {
            'ensemble_fake_probability': float(ensemble_prob),
            'clip_fake_probability': float(clip_prob),
            'laa_fake_probability': float(laa_prob),
            'is_deepfake': ensemble_prob > 0.5,
            'method': method,
            'num_frames_analyzed': len(frames),
            'laa_available': self.laa_available
        }


# Global detector instance (lazy initialization)
_detector_instance = None
_detector_lock = None

# Backward compatibility: Keep the old function name for existing code
def detect_fake_enhanced(video_path: str, **kwargs) -> Dict[str, Any]:
    """
    Enhanced deepfake detection using ensemble of CLIP and LAA-Net.
    Backward compatibility wrapper for existing code.
    Uses singleton pattern to reuse detector instance across calls.
    """
    global _detector_instance, _detector_lock
    
    # Use singleton to avoid reloading CLIP model on every call
    if _detector_instance is None:
        try:
            _detector_instance = EnhancedDetector(**kwargs)
        except Exception as e:
            # If initialization fails, try again with explicit CPU
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"EnhancedDetector initialization failed: {e}, retrying with CPU...")
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            _detector_instance = EnhancedDetector(**kwargs)
    
    try:
        result = _detector_instance.detect(video_path)
        
        # Convert to expected format for backward compatibility
        ensemble_prob = result['ensemble_fake_probability']
        return {
            'is_fake': result['is_deepfake'],
            'confidence': ensemble_prob if result['is_deepfake'] else (1 - ensemble_prob),
            'ensemble_score': ensemble_prob,
            'fake_probability': ensemble_prob,  # Explicit for API/frontend
            'authenticity_score': 1 - ensemble_prob,
            'detector_scores': {
                'clip_based': result['clip_fake_probability'],
                'laa_net': result['laa_fake_probability']
            },
            'method': result['method'],
            'video_hash': None,  # Can be added if needed
            'frame_count': result['num_frames_analyzed']
        }
    except Exception as e:
        # If detection fails, reset detector and retry once
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"Detection failed: {e}, resetting detector and retrying...")
        _detector_instance = None  # Reset singleton
        try:
            _detector_instance = EnhancedDetector(**kwargs)
            result = _detector_instance.detect(video_path)
            ep = result['ensemble_fake_probability']
            return {
                'is_fake': result['is_deepfake'],
                'confidence': ep if result['is_deepfake'] else (1 - ep),
                'ensemble_score': ep,
                'fake_probability': ep,
                'authenticity_score': 1 - ep,
                'detector_scores': {
                    'clip_based': result['clip_fake_probability'],
                    'laa_net': result['laa_fake_probability']
                },
                'method': result['method'],
                'video_hash': None,
                'frame_count': result['num_frames_analyzed']
            }
        except Exception as retry_e:
            # If retry also fails, raise the error
            logger.error(f"Detection failed even after retry: {retry_e}")
            raise retry_e


# Global detector instance for singleton pattern
_detector_instance = None

def get_enhanced_detector(**kwargs) -> EnhancedDetector:
    """Get or create enhanced detector instance (singleton)."""
    global _detector_instance
    if _detector_instance is None:
        _detector_instance = EnhancedDetector(**kwargs)
    return _detector_instance


if __name__ == "__main__":
    # Example usage and testing
    print("Testing Enhanced DeepFake Detector (Priority 1 MVP)...")
    print("=" * 60)
    
    # Initialize detector
    detector = EnhancedDetector()
    print(f"Detector initialized on device: {detector.device}")
    print(f"CLIP model: Loaded")
    print(f"LAA-Net: {'Available' if detector.laa_available else 'Not available (submodule setup required)'}")
    print()
    
    # Test with sample video if available
    sample_videos = [
        "sample_video.mp4",
        "test_video_1.mp4",
        "test_video_2.mp4",
        "test_video_3.mp4"
    ]
    
    test_video = None
    for video in sample_videos:
        if os.path.exists(video):
            test_video = video
            break
    
    if test_video:
        print(f"Testing with {test_video}...")
        try:
            result = detector.detect(test_video, num_frames=16)
            print("\nDetection Results:")
            print(f"  Method: {result['method']}")
            print(f"  Is Deepfake: {result['is_deepfake']}")
            print(f"  Ensemble Probability: {result['ensemble_fake_probability']:.4f}")
            print(f"  CLIP Probability: {result['clip_fake_probability']:.4f}")
            print(f"  LAA-Net Probability: {result['laa_fake_probability']:.4f}")
            print(f"  Frames Analyzed: {result['num_frames_analyzed']}")
        except Exception as e:
            print(f"Error during detection: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("No sample video found. Enhanced detector is ready for use.")
        print("\nTo test:")
        print("  detector = EnhancedDetector()")
        print("  result = detector.detect('path/to/video.mp4')")
        print("  print(result)")
