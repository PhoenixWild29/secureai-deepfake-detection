#!/usr/bin/env python3
"""
Test Suite for Work Order #40 Implementation
Dashboard API Response Models for Data Integration
"""

import pytest
from uuid import uuid4, UUID
from datetime import datetime, timezone
from typing import Dict, Any, List

# Import our implemented models
from src.api.schemas.dashboard_models import (
    DashboardOverviewResponse,
    DashboardAnalyticsResponse,
    UserPreferencesRequest,
    DashboardConfigurationResponse,
    DashboardWidgetType,
    NotificationType,
    ThemeType
)


class TestDashboardOverviewResponse:
    """Test the DashboardOverviewResponse model"""
    
    def test_dashboard_overview_response_creation(self):
        """Test creating a valid DashboardOverviewResponse"""
        response = DashboardOverviewResponse(
            user_summary={
                "total_analyses": 150,
                "success_rate": 95.5,
                "account_type": "premium"
            },
            recent_analyses=[
                {
                    "analysis_id": str(uuid4()),
                    "status": "completed",
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "confidence": 0.85
                }
            ],
            system_status={
                "overall_status": "healthy",
                "services": {
                    "detection_service": "operational",
                    "blockchain_service": "operational"
                }
            },
            quick_stats={
                "total_detections": 1250,
                "processing_time_avg": 2.5,
                "accuracy_rate": 96.2
            },
            notifications=[
                {
                    "id": str(uuid4()),
                    "type": "info",
                    "message": "System update available",
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            ]
        )
        
        assert response.user_summary["total_analyses"] == 150
        assert response.system_status["overall_status"] == "healthy"
        assert len(response.recent_analyses) == 1
        assert len(response.notifications) == 1
    
    def test_user_summary_validation(self):
        """Test user summary validation"""
        # Valid user summary
        valid_summary = {
            "total_analyses": 100,
            "success_rate": 90.0
        }
        response = DashboardOverviewResponse(
            user_summary=valid_summary,
            system_status={"overall_status": "healthy", "services": {}},
            quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
        )
        assert response.user_summary == valid_summary
        
        # Invalid user summary - missing required field
        with pytest.raises(ValueError, match="user_summary missing required field"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100},  # Missing success_rate
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
        
        # Invalid user summary - negative total_analyses
        with pytest.raises(ValueError, match="total_analyses must be a non-negative integer"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": -10, "success_rate": 90.0},
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
        
        # Invalid user summary - invalid success_rate
        with pytest.raises(ValueError, match="success_rate must be a number between 0.0 and 100.0"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100, "success_rate": 150.0},
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
    
    def test_recent_analyses_validation(self):
        """Test recent analyses validation"""
        # Valid recent analyses
        valid_analyses = [
            {
                "analysis_id": str(uuid4()),
                "status": "completed",
                "created_at": datetime.now(timezone.utc).isoformat()
            }
        ]
        response = DashboardOverviewResponse(
            user_summary={"total_analyses": 100, "success_rate": 90.0},
            recent_analyses=valid_analyses,
            system_status={"overall_status": "healthy", "services": {}},
            quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
        )
        assert len(response.recent_analyses) == 1
        
        # Invalid recent analyses - invalid status
        with pytest.raises(ValueError, match="status must be one of"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100, "success_rate": 90.0},
                recent_analyses=[
                    {
                        "analysis_id": str(uuid4()),
                        "status": "invalid_status",
                        "created_at": datetime.now(timezone.utc).isoformat()
                    }
                ],
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
    
    def test_system_status_validation(self):
        """Test system status validation"""
        # Valid system status
        valid_status = {
            "overall_status": "healthy",
            "services": {
                "detection_service": "operational",
                "blockchain_service": "operational"
            }
        }
        response = DashboardOverviewResponse(
            user_summary={"total_analyses": 100, "success_rate": 90.0},
            system_status=valid_status,
            quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
        )
        assert response.system_status["overall_status"] == "healthy"
        
        # Invalid system status - invalid overall_status
        with pytest.raises(ValueError, match="overall_status must be one of"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100, "success_rate": 90.0},
                system_status={"overall_status": "invalid_status", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
    
    def test_quick_stats_validation(self):
        """Test quick stats validation"""
        # Valid quick stats
        valid_stats = {
            "total_detections": 1000,
            "processing_time_avg": 2.5,
            "accuracy_rate": 95.0
        }
        response = DashboardOverviewResponse(
            user_summary={"total_analyses": 100, "success_rate": 90.0},
            system_status={"overall_status": "healthy", "services": {}},
            quick_stats=valid_stats
        )
        assert response.quick_stats["total_detections"] == 1000
        
        # Invalid quick stats - negative total_detections
        with pytest.raises(ValueError, match="total_detections must be a non-negative integer"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100, "success_rate": 90.0},
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": -100, "processing_time_avg": 1.0, "accuracy_rate": 90.0}
            )
    
    def test_notifications_validation(self):
        """Test notifications validation"""
        # Valid notifications
        valid_notifications = [
            {
                "id": str(uuid4()),
                "type": "info",
                "message": "Test notification",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        ]
        response = DashboardOverviewResponse(
            user_summary={"total_analyses": 100, "success_rate": 90.0},
            system_status={"overall_status": "healthy", "services": {}},
            quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0},
            notifications=valid_notifications
        )
        assert len(response.notifications) == 1
        
        # Invalid notifications - invalid type
        with pytest.raises(ValueError, match="type must be one of"):
            DashboardOverviewResponse(
                user_summary={"total_analyses": 100, "success_rate": 90.0},
                system_status={"overall_status": "healthy", "services": {}},
                quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0},
                notifications=[
                    {
                        "id": str(uuid4()),
                        "type": "invalid_type",
                        "message": "Test notification",
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                ]
            )
    
    def test_utility_methods(self):
        """Test utility methods"""
        response = DashboardOverviewResponse(
            user_summary={"total_analyses": 100, "success_rate": 90.0},
            system_status={"overall_status": "healthy", "services": {}},
            quick_stats={"total_detections": 100, "processing_time_avg": 1.0, "accuracy_rate": 90.0},
            notifications=[
                {"id": str(uuid4()), "type": "info", "message": "Test", "timestamp": datetime.now(timezone.utc).isoformat(), "read": False},
                {"id": str(uuid4()), "type": "warning", "message": "Test", "timestamp": datetime.now(timezone.utc).isoformat(), "read": True}
            ]
        )
        
        assert response.get_total_notifications() == 2
        assert response.get_unread_notifications() == 1
        assert response.is_system_healthy() is True


class TestDashboardAnalyticsResponse:
    """Test the DashboardAnalyticsResponse model"""
    
    def test_dashboard_analytics_response_creation(self):
        """Test creating a valid DashboardAnalyticsResponse"""
        response = DashboardAnalyticsResponse(
            performance_trends={
                "processing_time_trend": {
                    "data_points": [
                        {"timestamp": "2024-01-01", "value": 2.5}
                    ]
                },
                "accuracy_trend": {
                    "data_points": [
                        {"timestamp": "2024-01-01", "value": 95.0}
                    ]
                }
            },
            usage_metrics={
                "total_users": 1000,
                "active_users": 250,
                "analyses_count": 5000
            },
            confidence_distribution={
                "bins": {"0.0-0.2": 100, "0.2-0.4": 150},
                "statistics": {"mean": 0.6, "median": 0.65, "std_dev": 0.15}
            },
            processing_metrics={
                "average_processing_time": 2.5,
                "throughput": 100.0,
                "success_rate": 95.0
            }
        )
        
        assert response.usage_metrics["total_users"] == 1000
        assert response.processing_metrics["success_rate"] == 95.0
        assert "pdf" in response.export_options
    
    def test_performance_trends_validation(self):
        """Test performance trends validation"""
        # Valid performance trends
        valid_trends = {
            "processing_time_trend": {
                "data_points": [
                    {"timestamp": "2024-01-01", "value": 2.5}
                ]
            },
            "accuracy_trend": {
                "data_points": [
                    {"timestamp": "2024-01-01", "value": 95.0}
                ]
            }
        }
        response = DashboardAnalyticsResponse(
            performance_trends=valid_trends,
            usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
            confidence_distribution={"bins": {}, "statistics": {}},
            processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
        )
        assert "processing_time_trend" in response.performance_trends
        
        # Invalid performance trends - missing required field
        with pytest.raises(ValueError, match="performance_trends missing required field"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}},  # Missing accuracy_trend
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
            )
    
    def test_usage_metrics_validation(self):
        """Test usage metrics validation"""
        # Valid usage metrics
        valid_metrics = {
            "total_users": 1000,
            "active_users": 250,
            "analyses_count": 5000
        }
        response = DashboardAnalyticsResponse(
            performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
            usage_metrics=valid_metrics,
            confidence_distribution={"bins": {}, "statistics": {}},
            processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
        )
        assert response.usage_metrics["total_users"] == 1000
        
        # Invalid usage metrics - negative value
        with pytest.raises(ValueError, match="must be a non-negative integer"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": -100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
            )
    
    def test_confidence_distribution_validation(self):
        """Test confidence distribution validation"""
        # Valid confidence distribution
        valid_distribution = {
            "bins": {"0.0-0.2": 100, "0.2-0.4": 150},
            "statistics": {"mean": 0.6, "median": 0.65, "std_dev": 0.15}
        }
        response = DashboardAnalyticsResponse(
            performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
            usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
            confidence_distribution=valid_distribution,
            processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
        )
        assert "bins" in response.confidence_distribution
        
        # Invalid confidence distribution - invalid statistics range
        with pytest.raises(ValueError, match="must be between 0.0 and 1.0"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {"mean": 1.5}},  # Invalid mean
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0}
            )
    
    def test_processing_metrics_validation(self):
        """Test processing metrics validation"""
        # Valid processing metrics
        valid_metrics = {
            "average_processing_time": 2.5,
            "throughput": 100.0,
            "success_rate": 95.0
        }
        response = DashboardAnalyticsResponse(
            performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
            usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
            confidence_distribution={"bins": {}, "statistics": {}},
            processing_metrics=valid_metrics
        )
        assert response.processing_metrics["average_processing_time"] == 2.5
        
        # Invalid processing metrics - negative processing time
        with pytest.raises(ValueError, match="must be a non-negative number"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": -1.0, "throughput": 50.0, "success_rate": 90.0}
            )
    
    def test_export_options_validation(self):
        """Test export options validation"""
        # Valid export options
        valid_options = ["pdf", "json", "csv"]
        response = DashboardAnalyticsResponse(
            performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
            usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
            confidence_distribution={"bins": {}, "statistics": {}},
            processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0},
            export_options=valid_options
        )
        assert response.export_options == valid_options
        
        # Invalid export options - unsupported format
        with pytest.raises(ValueError, match="must be one of"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0},
                export_options=["pdf", "invalid_format"]
            )
    
    def test_analytics_period_validation(self):
        """Test analytics period validation"""
        # Valid periods
        valid_periods = ["1d", "7d", "30d", "90d", "1y", "all"]
        for period in valid_periods:
            response = DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0},
                analytics_period=period
            )
            assert response.analytics_period == period
        
        # Invalid period
        with pytest.raises(ValueError, match="analytics_period must be one of"):
            DashboardAnalyticsResponse(
                performance_trends={"processing_time_trend": {"data_points": []}, "accuracy_trend": {"data_points": []}},
                usage_metrics={"total_users": 100, "active_users": 50, "analyses_count": 500},
                confidence_distribution={"bins": {}, "statistics": {}},
                processing_metrics={"average_processing_time": 1.0, "throughput": 50.0, "success_rate": 90.0},
                analytics_period="invalid_period"
            )
    
    def test_utility_methods(self):
        """Test utility methods"""
        response = DashboardAnalyticsResponse(
            performance_trends={
                "processing_time_trend": {
                    "data_points": [
                        {"timestamp": "2024-01-01", "value": 2.5},
                        {"timestamp": "2024-01-02", "value": 3.0}
                    ]
                },
                "accuracy_trend": {
                    "data_points": [
                        {"timestamp": "2024-01-01", "value": 95.0}
                    ]
                }
            },
            usage_metrics={"total_users": 1000, "active_users": 250, "analyses_count": 5000},
            confidence_distribution={"bins": {}, "statistics": {"mean": 0.6}},
            processing_metrics={"average_processing_time": 2.5, "throughput": 100.0, "success_rate": 95.0}
        )
        
        assert response.get_total_data_points() == 3
        assert "pdf" in response.get_available_export_formats()
        assert response.get_confidence_statistics() == {"mean": 0.6}
        
        summary = response.get_performance_summary()
        assert summary["average_processing_time"] == 2.5
        assert summary["total_users"] == 1000


class TestUserPreferencesRequest:
    """Test the UserPreferencesRequest model"""
    
    def test_user_preferences_request_creation(self):
        """Test creating a valid UserPreferencesRequest"""
        request = UserPreferencesRequest(
            layout_config={
                "widgets": [
                    {
                        "type": "overview",
                        "position": {"x": 0, "y": 0},
                        "size": {"width": 400, "height": 300}
                    }
                ]
            },
            notification_settings={
                "enabled_types": ["info", "warning"],
                "frequency": "daily"
            },
            theme_preferences={
                "theme": "light",
                "font_size": 14,
                "color_scheme": "default"
            },
            analytics_filters={
                "date_range": {"period": "30d"},
                "selected_metrics": ["processing_time", "accuracy"]
            }
        )
        
        assert len(request.layout_config["widgets"]) == 1
        assert "info" in request.notification_settings["enabled_types"]
        assert request.theme_preferences["theme"] == "light"
    
    def test_layout_config_validation(self):
        """Test layout configuration validation"""
        # Valid layout config
        valid_config = {
            "widgets": [
                {
                    "type": "overview",
                    "position": {"x": 0, "y": 0}
                }
            ]
        }
        request = UserPreferencesRequest(layout_config=valid_config)
        assert len(request.layout_config["widgets"]) == 1
        
        # Invalid layout config - invalid widget type
        with pytest.raises(ValueError, match="type must be one of"):
            UserPreferencesRequest(
                layout_config={
                    "widgets": [
                        {
                            "type": "invalid_type",
                            "position": {"x": 0, "y": 0}
                        }
                    ]
                }
            )
        
        # Invalid layout config - missing position
        with pytest.raises(ValueError, match="missing required field"):
            UserPreferencesRequest(
                layout_config={
                    "widgets": [
                        {
                            "type": "overview"
                            # Missing position
                        }
                    ]
                }
            )
    
    def test_notification_settings_validation(self):
        """Test notification settings validation"""
        # Valid notification settings
        valid_settings = {
            "enabled_types": ["info", "warning", "error"],
            "frequency": "hourly"
        }
        request = UserPreferencesRequest(notification_settings=valid_settings)
        assert "info" in request.notification_settings["enabled_types"]
        
        # Invalid notification settings - invalid type
        with pytest.raises(ValueError, match="must be one of"):
            UserPreferencesRequest(
                notification_settings={
                    "enabled_types": ["invalid_type"],
                    "frequency": "daily"
                }
            )
        
        # Invalid notification settings - invalid frequency
        with pytest.raises(ValueError, match="frequency must be one of"):
            UserPreferencesRequest(
                notification_settings={
                    "enabled_types": ["info"],
                    "frequency": "invalid_frequency"
                }
            )
    
    def test_theme_preferences_validation(self):
        """Test theme preferences validation"""
        # Valid theme preferences
        valid_theme = {
            "theme": "dark",
            "font_size": 16,
            "color_scheme": "high_contrast"
        }
        request = UserPreferencesRequest(theme_preferences=valid_theme)
        assert request.theme_preferences["theme"] == "dark"
        
        # Invalid theme preferences - invalid theme
        with pytest.raises(ValueError, match="theme must be one of"):
            UserPreferencesRequest(
                theme_preferences={
                    "theme": "invalid_theme",
                    "font_size": 14
                }
            )
        
        # Invalid theme preferences - invalid font size
        with pytest.raises(ValueError, match="font_size must be between 10 and 24"):
            UserPreferencesRequest(
                theme_preferences={
                    "theme": "light",
                    "font_size": 5  # Too small
                }
            )
    
    def test_analytics_filters_validation(self):
        """Test analytics filters validation"""
        # Valid analytics filters
        valid_filters = {
            "date_range": {"period": "7d"},
            "selected_metrics": ["processing_time", "accuracy", "throughput"]
        }
        request = UserPreferencesRequest(analytics_filters=valid_filters)
        assert request.analytics_filters["date_range"]["period"] == "7d"
        
        # Invalid analytics filters - invalid period
        with pytest.raises(ValueError, match="period must be one of"):
            UserPreferencesRequest(
                analytics_filters={
                    "date_range": {"period": "invalid_period"},
                    "selected_metrics": ["processing_time"]
                }
            )
        
        # Invalid analytics filters - invalid metric
        with pytest.raises(ValueError, match="must be one of"):
            UserPreferencesRequest(
                analytics_filters={
                    "date_range": {"period": "30d"},
                    "selected_metrics": ["invalid_metric"]
                }
            )
    
    def test_utility_methods(self):
        """Test utility methods"""
        request = UserPreferencesRequest(
            layout_config={
                "widgets": [
                    {"type": "overview", "position": {"x": 0, "y": 0}},
                    {"type": "analytics", "position": {"x": 400, "y": 0}}
                ]
            },
            notification_settings={
                "enabled_types": ["info", "warning"],
                "frequency": "daily"
            },
            theme_preferences={
                "theme": "dark",
                "font_size": 16,
                "color_scheme": "high_contrast"
            },
            analytics_filters={
                "date_range": {"period": "30d"},
                "selected_metrics": ["processing_time", "accuracy"]
            }
        )
        
        assert len(request.get_enabled_notification_types()) == 2
        assert len(request.get_selected_widgets()) == 2
        assert request.get_theme_settings()["theme"] == "dark"
        assert request.get_analytics_date_range() == "30d"
        assert request.is_notification_enabled("info") is True
        assert request.has_widget("overview") is True
        assert request.has_widget("nonexistent") is False


class TestDashboardConfigurationResponse:
    """Test the DashboardConfigurationResponse model"""
    
    def test_dashboard_configuration_response_creation(self):
        """Test creating a valid DashboardConfigurationResponse"""
        response = DashboardConfigurationResponse(
            available_widgets=[
                {
                    "type": "overview",
                    "name": "Overview Widget",
                    "description": "Main dashboard overview"
                },
                {
                    "type": "analytics",
                    "name": "Analytics Widget",
                    "description": "Performance analytics"
                }
            ],
            default_layout={
                "widgets": [
                    {"type": "overview", "position": {"x": 0, "y": 0}}
                ]
            },
            theme_options=[
                {"name": "light", "display_name": "Light Theme"},
                {"name": "dark", "display_name": "Dark Theme"}
            ],
            notification_options={
                "types": ["info", "warning", "error"],
                "delivery_methods": ["email", "in_app"]
            },
            analytics_options={
                "date_ranges": ["1d", "7d", "30d"],
                "metrics": ["processing_time", "accuracy"]
            }
        )
        
        assert len(response.available_widgets) == 2
        assert response.get_available_widget_types() == ["overview", "analytics"]
        assert response.get_widget_by_type("overview") is not None
        assert response.get_widget_by_type("nonexistent") is None


class TestEnums:
    """Test the enumeration classes"""
    
    def test_dashboard_widget_type_enum(self):
        """Test DashboardWidgetType enum values"""
        assert DashboardWidgetType.OVERVIEW == "overview"
        assert DashboardWidgetType.ANALYTICS == "analytics"
        assert DashboardWidgetType.RECENT_ACTIVITY == "recent_activity"
        assert DashboardWidgetType.SYSTEM_STATUS == "system_status"
    
    def test_notification_type_enum(self):
        """Test NotificationType enum values"""
        assert NotificationType.INFO == "info"
        assert NotificationType.WARNING == "warning"
        assert NotificationType.ERROR == "error"
        assert NotificationType.SUCCESS == "success"
    
    def test_theme_type_enum(self):
        """Test ThemeType enum values"""
        assert ThemeType.LIGHT == "light"
        assert ThemeType.DARK == "dark"
        assert ThemeType.AUTO == "auto"
        assert ThemeType.HIGH_CONTRAST == "high_contrast"


def test_work_order_40_requirements_compliance():
    """
    Test that all Work Order #40 requirements are met
    """
    
    # Test 1: DashboardOverviewResponse model with all required fields
    overview_response = DashboardOverviewResponse(
        user_summary={
            "total_analyses": 150,
            "success_rate": 95.5
        },
        recent_analyses=[
            {
                "analysis_id": str(uuid4()),
                "status": "completed",
                "created_at": datetime.now(timezone.utc).isoformat()
            }
        ],
        system_status={
            "overall_status": "healthy",
            "services": {
                "detection_service": "operational"
            }
        },
        quick_stats={
            "total_detections": 1250,
            "processing_time_avg": 2.5,
            "accuracy_rate": 96.2
        },
        notifications=[
            {
                "id": str(uuid4()),
                "type": "info",
                "message": "System update available",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        ],
        preferences={
            "theme": "light",
            "layout": "default"
        }
    )
    
    # Verify all required fields are present
    assert hasattr(overview_response, 'user_summary')
    assert hasattr(overview_response, 'recent_analyses')
    assert hasattr(overview_response, 'system_status')
    assert hasattr(overview_response, 'quick_stats')
    assert hasattr(overview_response, 'notifications')
    assert hasattr(overview_response, 'preferences')
    
    # Test 2: DashboardAnalyticsResponse model with all required fields
    analytics_response = DashboardAnalyticsResponse(
        performance_trends={
            "processing_time_trend": {
                "data_points": [
                    {"timestamp": "2024-01-01", "value": 2.5}
                ]
            },
            "accuracy_trend": {
                "data_points": [
                    {"timestamp": "2024-01-01", "value": 95.0}
                ]
            }
        },
        usage_metrics={
            "total_users": 1000,
            "active_users": 250,
            "analyses_count": 5000
        },
        confidence_distribution={
            "bins": {"0.0-0.2": 100},
            "statistics": {"mean": 0.6}
        },
        processing_metrics={
            "average_processing_time": 2.5,
            "throughput": 100.0,
            "success_rate": 95.0
        },
        export_options=["pdf", "json", "csv"]
    )
    
    # Verify all required fields are present
    assert hasattr(analytics_response, 'performance_trends')
    assert hasattr(analytics_response, 'usage_metrics')
    assert hasattr(analytics_response, 'confidence_distribution')
    assert hasattr(analytics_response, 'processing_metrics')
    assert hasattr(analytics_response, 'export_options')
    
    # Test 3: UserPreferencesRequest model with all required fields
    preferences_request = UserPreferencesRequest(
        layout_config={
            "widgets": [
                {
                    "type": "overview",
                    "position": {"x": 0, "y": 0}
                }
            ]
        },
        notification_settings={
            "enabled_types": ["info", "warning"],
            "frequency": "daily"
        },
        theme_preferences={
            "theme": "light",
            "font_size": 14
        },
        analytics_filters={
            "date_range": {"period": "30d"},
            "selected_metrics": ["processing_time", "accuracy"]
        }
    )
    
    # Verify all required fields are present
    assert hasattr(preferences_request, 'layout_config')
    assert hasattr(preferences_request, 'notification_settings')
    assert hasattr(preferences_request, 'theme_preferences')
    assert hasattr(preferences_request, 'analytics_filters')
    
    # Test 4: Proper field validation using SQLModel integration patterns
    # This is implicitly tested through the Field usage and validators in the model definitions
    # The models use Field with proper descriptions and validation constraints
    
    # Test 5: JSON serialization support
    # Test that models can be serialized to JSON
    overview_json = overview_response.model_dump()
    analytics_json = analytics_response.model_dump()
    preferences_json = preferences_request.model_dump()
    
    assert isinstance(overview_json, dict)
    assert isinstance(analytics_json, dict)
    assert isinstance(preferences_json, dict)
    
    # Test 6: Integration with existing BaseModel patterns
    # All models inherit from BaseModel and use consistent patterns
    
    print("âœ… All Work Order #40 requirements have been successfully implemented and tested!")


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v"])
